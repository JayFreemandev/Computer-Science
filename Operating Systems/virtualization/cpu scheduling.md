## Too good to be true   
CPU 스케줄링설명에서 사용될 가정들은 매우 이상적이며 현실에서 일어날수없다.   
내용이 심화됨에 따라 가정을 현실적으로 바꿔가며 마지막에는 올바른 스케줄링을 느끼게될거다.    
  
다음과 같은 비현실적인 가정을 해보자.  
모든 작업은 동시에 도착하며 같은 시간 동안 실행된다.   
각 작업은 끝날때까지 실행하며 입출력을 배제한 CPU만 사용한다.    

First In First Out, FIFO   
![Untitled](https://user-images.githubusercontent.com/72185011/175782665-6a2df5ac-e370-4345-b149-d46987307a5e.png)
<br>

A,B,C 동시 도착에 A,B,C 순서대로 도착했고 10초동안 실행되었다.   
평균 반환값은 A+B+C/3 인 = 20이다. 계산은 쉽다. 현실적인 가정으로 돌아가보자.   
작업 실행 시간이 똑같지않다라고 하면 FIFO의 미래는?     
![Untitled 1](https://user-images.githubusercontent.com/72185011/175782669-5dca27e2-bd2f-42d1-917f-9c9262aedcc4.png)
<br>

보다싶이 A가 100초나 먹고 B, C 까지 다시 계산하면 평균 반환시간은 110초다. 이런 문제점을 Convoy effect라고 한다.    
5인큐를 돌렸는데 1명이 로딩이 너무 느려서 1%남기고 99%까지 채운 나머지 4명이 고통을 받는다.   
작업 실행 시간이 다르다는 현실적인 가정이면 무슨 알고리즘이 베스트인가?  
<br>
Shortest Job First, SJF   
![Untitled 2](https://user-images.githubusercontent.com/72185011/175782676-6e3c1702-dd1e-4945-b620-4aa1affc6292.png)

젤 작업시간 많이 차지하는놈을 마지막에 처리하고 짧게 끝나는 작업들을 먼저 처리하는거다.     
10+20+120/3만 해도 50초다. 110초에 비하면 2배 넘게 빨라졌다.   
그럼 다시 모든 작업이 동시에 도착하지 않는다는 현실적인 가정으로 업그레이드 한다면?   
A가 t=0에 제일 빠르게 도착하고 100초동안 돌아가고 B와 C는 t=10에 도착하고 10초동안 돌아간다고 생각하면   
B와 C가 A 바로 뒤에 도착해도. 일단 제일 먼저 도착한 A가 또 다시 100초동안 convoy effect를 일으킨다.  
  
### 쉬어가자.  
옛날 시절에는 작업이 종료될 때까지 계속 실행하는 non-preemptive 비선점형 스케줄러를 사용했는데  
**현대 시대에는 대부분 선점 스케줄러를 사용**한다. 어떻게?   
현재 프로세스 중단하고 Context Switch를 통해 다른 프로세스 실행시키면서.   

## Shortest Time-to-Completion First, STCF

SJF는 비선점 스케줄러라 끝날때까지 아무것도 못하지만 우리가   
Context Switch나 타이머 인터럽트를 생각해본다면 다시 현실적인 가정을 해보자.     
A를 중단 시키고 B나 C를 실행할수도 있다.   

B와 C도착할때까지만 A일을 하다가 바로 뺏들어어서 B, C 10초씩 일 끝내고 다시 A돌리면 되지않는가?   
그렇기에 최적의 스케줄링이라 한다. 하지만 초기 컴퓨팅에서만 인정받고 Time Sharing이 등장하며 뒤바뀌게 되었다.  

앞으로는 평균 반환값으로 평가되는게 아니라 응답 시간으로 평가된다.   
A,B,C 가 동시에 도착하는 경우 C는 한 번 작업하기위해 A와 B가 끝날때까지 기다린다.  
평균 반환 시간은 좋지만 응답 시간은 좋다고 할수가 없다.   

“명령어를 입력하고 시스템으로부터 응답을 받기까지 10초 기다리는걸 상상해보자   
그저 다른 작업이 내 명령어보다 먼저 스케줄링 되었다는 이유로 말이다.  

## Round-Robin, RR
![1901___2013927395](https://user-images.githubusercontent.com/72185011/175782682-8d9d0bbf-8935-4534-b86e-20ce0b394554.gif)
<br>
쎈시티브한 응답시간을 위해 라운드 로빈이라는 스케줄링 알고리즘이 등장한다.   
사나이 라운드 로빈은 기다리지 않는다. 작업이 실행하는 시간을 “**타임 슬라이스**” 라고 한다.   
타임 슬라이스별로 짤라버리는것이다. 일정 시간 실행 후 다음 작업으로 전환해버린다.   
![Untitled 3](https://user-images.githubusercontent.com/72185011/175782703-f75ca3c4-1baa-49ea-a7da-5d12a60667ad.png)
<br>
SFJ보면 A,B,C 5초씩 1초마다 타임슬라이스 해버리는 라운드로빈은 초단위로 작업을 빠르게 왔다 갔다하기 시작한다.   
라운드 로빈의 평균 응답 시간은 0+1+2 / 3 = 1이고 SJF는 0+5+10/3 = 5이다.   

타임 슬라이스가 짧아질수록 반환 성능은 좋아지지만 너무 짧게도 너무 길게도 잡아서는 안된다.   
Context Switch는 저장한 레지스터를 꺼내오는 작업말고 CPU 캐시, TLB 등   
다양한 작업 정보 저장이 들어있기에 이런 작업은 적지 않은 비용이 발생할 수 밖에 없다.  

## No More Oracle

평균 반환값은 빠르지만 응답 시간은 느린 SJF, STCF. 응답 시간은 빠르지만 반환 시간은 느린 RR.  
서론에 말한 이상적인 가정들을 조금씩 풀어보자.   
입출력없는 프로그램은 의미가없으며 모든 시간들을 미리 내려다보고 결과를 알고 실행하는 스케줄러는 존재하지않는다.   

입출력이 더해지는 순간 프로세스는 가장 짧은 작업을 선택하며 왔다 갔다 오갈것이다.   
이렇듯 서로 역설적인 상황이며 미래를 내다볼수없는 운영체제의 문제는 해결할수없었다.   

![Untitled 4](https://user-images.githubusercontent.com/72185011/175782603-9ab51af5-913e-4966-8e82-df190e9004fd.png)  
하지만 가까운 과거를 통해 미래를 예측한다. “**멀티 레벨 피드백 큐**” 자료구조가 등장한다.     
<br>
## Multi-level Feedback Queue, MLFQ

짧은 작업 먼저끝내고 반환시간 줄인다. (SJF, STCF)  
스크린을 쳐다보며 기다리는 사용자를 위해 응답 시간을 최적화한다.  

MLFQ는 큐로 이루어지며 우선순위(priority level)을 따른다.   
여러 단계의 큐를 피드백으로 우선순위를 정하게된다.   
과거의 결과가 우선순위를 결정하게된다.  

긴 작업과 짧은 작업을 잘 처리하고 입출력이 섞인 대화형 작업에서도   
좋은 응답 시간을 보여주기에 완벽하게 보이지만 문제점이 없는것은 아니다.   

기아(starvation)을 일으킨다. 왜냐 시스템에 대화형 작업이 많아지면  
그 작업들이 CPU다 잡아먹고 나머지 작업들은 할당받지못해 기아상태로 굶어 죽는다.   

공격에 취약하다. 이게 무슨소리냐? 어차피 타임 슬라이스로 여러 작업들을 오가며 수행하게된다.     
만약에 타임슬라이스 끝나기전에 악의적인 의도로 입출력 요청을 보내면?   
나는 스케줄러를 속여서 시간을 더 할당 받게된다.   
99%의 타임 슬라이스를 실행후 CPU를 받게되면 “**독점**” 할수있다  

문제를 해결하기 위한 방법으로는 다음과 같다.   
기아상태를 해결하기 위해 모든 작업을 최상위큐로 보낸다 S시간 이후에   
최상위 큐로 보내는 의미는 높은 우선순위 작업들을 CPU와 공유하게되서 프로세스가 굶지 않는다.   

### 흑마술

그 특정 S시간을 결정하는게 중요한데 얼마로 해야할까?   
덕망있는 연구원 John Ousterhout는 S값을 “voodoo constants”라고 언급했다.   
정확한 S값을 뽑아 내기위해 흑마술이 필요하다.   
너무 길면 기아상태가 발생하고 너무 짧으면 대화형 작업에서 CPU할당이 부족하다.  

MLFQ은 우선순위를 비교했을때 높은 작업만 실행하고 같으면 라운드로빈으로 타임슬라이스 방식으로 작업을 처리하게된다.   
기아 상태를 해결하기위해 작업을 최상위 큐에 배치하며 처리하는 알고리즘이고   
대부분의 현대 운영체제인 window에서도 기본 스케줄러로 사용중이다.    

MLFQ는 정리하면서 딥하게 적지않았다.   
CPU의 스케줄링 알고리즘으로는 이런게 있다라는것만 짚고 넘어가고 싶기때문이다.  

## 어떻게 CPU를 정해진 비율로 값을 공유할 수 있는가?

![Untitled 5](https://user-images.githubusercontent.com/72185011/175782610-5379444e-edd8-4607-b433-9f6c0d1d7d9a.png)  
<br>  

Proportional Share라고 하는 개념을 다룬다.     
fair share 라는 공정 공유? 라고도 부르는데 fair share라고 하겠다.   
반환 시간 응답시간 최적화 대신 스케줄러가 각 작업에게 공정하게 값을 주는것이 목적인 개념이다.  

좋은 예시가 LOTTERY 로또 스케줄링이다. 간단하게 설명하자면 더 자주 쓰는 프로세스에게는 로또 당첨 기회를 더 많이 줘야된다.   
A와 B두 프로세스가 있다고 치면 A는 75장 B는 25장의 스피드 토토를 샀다하자.   
이 때 스케줄러는 몇 장의 복권을 샀는지 알아야한다.  

![Untitled 6](https://user-images.githubusercontent.com/72185011/175782614-06ca8657-b361-455f-ba2f-4e0cc96cbf70.png)  
<br>  
로또 스케줄링은 랜덤으로 결정하는데 운영체제를 보면 이렇게 랜덤으로 결정짓는것들이 있다   
딱 정하지 않고 왜 랜덤 난수를 돌려서 선택할까? **세가지 장점** 때문에 랜덤 결정을 사용한다.  

메모리 교체 알고리즘중에 LRU라는게 있는데 순차적으로 반복되는 알고리즘이라 성능이 끔찍하게 좋지 않지만    
랜덤으로 결정하는 알고리즘에서는 이런 **최악의 성능**을 막을수있다.   

프로세스의 상태값만을 요구하기때문에 관리해야 할 정보가 없다(위 예시처럼 전체 복권수는 알아야함)   
그래서 다른 알고리즘에 비해 **가볍다**.  

랜덤 난수로 결정하면 속도가 필요한 상황에서 **빠른 결정**을 보여준다.   
속도 증가를 위해 랜덤 난수를 어느 정도 필요한 곳만 랜덤으로 만들기도한다.  

위의 예시를 보면 랜덤 결정이라 25% 정확하게 보장하지않는다.  
B는 20개중 4개. 즉 20%를 희득했다. 우리가 산 25%와는 다르게 말이다.   
장시간 진행 될 수록 원하는 비율에 가까워지게 되있다.   
 
내가 구하고자 하는 범위 안에서 랜덤 난수를 선택하는건 어려운 일이다.  
하지만 정확한 결과를 보장하기위해 stride 스케줄링이라는 방식도 고안되었다.   
I/O 작업과 같은 대화형 작업이 병행 되었을때 랜덤이라 제대로 동작 안할수도있고     
복권을 얼마만큼 지정할지에 대해 정해진 답이 없기에 CPU 스케줄러로 잘 사용 안한다.   
이런 스케줄링이 존재하더라.     

## 멀티프로세서 스케줄링

요즘 컴퓨터들 다 잘나와서 여러개의 CPU 코어가 하나의 칩으로 박혀있는 멀티코어가 대중화되어있다.   
다중 CPU 시대가 열리며 병행성이라는 문제점이 떠오르기 시작했다.   
CPU 몇 개 더 단다고 싱글 어플리케이션의 성능이 빨라지는것이 아니다.   
어플리케이션을 스레드를 사용한 병렬로 짤수있을때 성능이 좋아진다.  

이건 프로그램 문제고 운영체제에서는 스케줄링 문제를 다룰것이다.    
지금까지는 단일 프로세서 스케줄링에 대해서만 이야기했는데 여러 CPU를 어떻게 스케줄링 해야하는가?  

멀티프로세서가 가지고있는 새로운 이슈들을 이해하기전에   
우리는 근본적으로 싱글 cpu 하드웨어와 멀티 cpu하드웨어가 뭐가 다른지부터 알아야된다.   

하드웨어 중앙에 위치한 caches 와 어떻게 데이터가 다중 프로세스에게 전달되는지가 다르다.   
더 깊게 알아보려면 대학원 이상의 컴퓨터 아키텍쳐 수업을 들어야할거다.   

싱글 CPU를 가진 시스템에서일 반적으로 프로세서가 프로그램을 빠르게 실행하는데 도움을 주는 하드웨어 caches의 계층구조가 있다.   
caches는 작으면서, 메인 메모리 시스템에서 자주 사용하는 데이터들의 복사본을 쥐고있는 성능좋은 메모리다.  

반면에 메인 메모리는 모든 데이터들을 다 쥐고있다 그래서 이렇게 큰 메모리에 접근하는거는 당연히 느리다.   
자주 접근하는 데이터들을 캐쉬에 둠으로써 시스템은 크고 느린 메모리를 빠르게 보이게한다.  
  
다중 CPU에 맞게 multi-queue multiprocessor scheduling, MQMS 방식이 나올 수 밖에없다.   
작업 시작시 하나의 스케줄링 큐에 집어 넣고 모두 독립적으로 작업을 진행시켜서 동기화를 방지한다.  

하나를 해결하니 다른 하나의 문제점이 부각된다.  **로드 불균형**(load imbalance)이다.   
4개의 작업을 2개의 CPU에서 시작하고 그 중 하나 C가 종료되었다면 스케줄링은 이런 그림이다.  

![Untitled 9](https://user-images.githubusercontent.com/72185011/175782637-16fd0398-b1fc-4c79-9e7f-ef6593f548a8.png)  
<br>

각 큐마다 라운드 로빈으로 스케줄러를 처리하면?   
![Untitled 10](https://user-images.githubusercontent.com/72185011/175782633-ef42cafb-ef93-4584-a693-dbf8902124df.png)
 
<br>
균일하게 처리되는것이 아니라 A가 B와D의 두배이상 차지하고있다. A와 C를 따라 종료해버린다면 B와 D만 남게되는데
![Untitled 11](https://user-images.githubusercontent.com/72185011/175782626-4d055d8d-0f5d-4cf8-88a2-634cb698ea61.png)
![Untitled 12](https://user-images.githubusercontent.com/72185011/175782629-971c6f66-c791-4a4f-b29e-7c8fdb9306eb.png)


위와 같은 로드 불균형이 발생한다. 이러한 불균형을 맞추기 위해서 여러 CPU를 오가면서 작업을 진행하여 균형을 맞추게된다.
큐를 검사해서 가득 차있다면 다른 CPU가 작업을 맡아서 일을 처리한다. 물론 큐를 자주 검사하게 되면 오버헤드가 일어남. 
<br>
큐를 얼마나 검사해야되는지는 신도 모른다.   
전에 설명한 얼만큼 얼마나가 절절한지는 부두술해서 값 찾는거 말고는 존재하지않는다.  
모든 현대 os들의 숙제다.  
<br>
싱글 큐와 멀티 큐와 같이 멀티 프로세서에서 사용할수있는 스케줄링이 있었다.   
싱글큐는 씸플하고 로드 불균형은 없지만 CPU 늘어나면 성능박살나버리고     
멀티큐는 CPU늘어나도 좋지만 로드 불균형이 발생할수있고 구현도 복잡하다.   
CPU 스케줄러의 마이너한 수정은 시스템에 큰 영향을 줄수있어서 만능 국밥 스케줄러 구현은 힘들다.  
<br>
“어떤 스케줄러를 사용해야하는지는 현재까지 뜨거운 감자며 모든것은 권장사항이자 관점에 따라 다르다.   
실용적이여야 한다 그러면 모든 문제가 쉽고 간단한 해결책을 가지지 않는다고 생각하기 때문이다.”  
