# script

프로세스가 무엇인지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 프로세스는 실행 중인 프로그램으로 디스크로부터 메모리에 적재되어 CPU의 할당을 받을 수 있는 상태를 말합니다. 프로세스는 운영체제부터 주소공간, 파일, 메모리 등을 할당 받습니다.
    - 스택 영역 : 지역변수, 매개변수, 리턴값 등이 임시로 저장되는 고정된 크기의 공간입니다. 주로 함수가 불렸을 때 데이터를 저장했다가 함수가 종료될 때 데이터를 반환합니다.
    - 힙 영역 : 프로세스가 동작 중에 동적으로 데이터를 할당하는 공간입니다.
    - 데이터 영역 : 데이터 영역에는 프로그램이 시작될 때 생성되는 전역변수, 정적변수 등이 저장됩니다.
        - `Blocked State Symbol` 영역 : 데이터 영역에는 BSS 영역이 포함되어 있습니다. BSS 영역은 초기화 되지 않은 정적변수와 전역변수가 저장됩니다. 초기화 되지 않은 변수들은 값을 저장할 필요가 없기 때문에 공간의 낭비를 줄이기 위해 BSS 영역을 구분합니다.
    - 코드 : 코드영역은 프로그램의 명령을 저장합니다.
    
    ***왜 이렇게 구역을 나눈건가요?***
    
    최대한 데이터를 공유하여 메모리 사용량을 줄여야 합니다.
    
    Code는 같은 프로그램 자체에서는 모두 같은 내용이기 때문에 따로 관리하여 공유함
    
    Stack과 데이터를 나눈 이유는, 스택 구조의 특성과 전역 변수의 활용성을 위한 것!
</details> 

프로세스의 상태에 대해 알고있는지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - NEW : 새롭게 생성된 프로세스가 가지는 상태입니다.
    - READY : 준비 큐에서 운영체제에 의해 CPU에 로드되길 기다리는 상태입니다.
    - RUNNING : CPU에 로드되어 실행 중인 상태입니다.
    - WAITING : I/O 이벤트가 발생했을 때 해당 이벤트가 처리되는 동안, 혹은 어떤 이벤트를 기다릴 때 Device queue 에서 대기하는 상태 입니다.
    - TERMINATED : 프로세스가 종료되고 CPU 에서 제거되었을 때의 상태입니다.
</details>

인터럽트와 트랩의 차이는?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 인터럽트는 하드웨어에 의해 발생됩니다. 하드웨어 장치가 CPU를 사용해야할 때, 인터럽트를 발생시킵니다.
    - 트랩은 소프트웨어에 의해 발생됩니다. 어떤 소프트웨어가 CPU에게 소프트웨어 동작에 필요한 동작을 요구할 때 트랩을 발생시킵니다.
</details>
   
시스템 콜은 뭔지 아는지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 시스템 콜은 프로세스가 트랩을 발생시킬 수 있도록 하기위해 OS에서 제공하는 인터페이스입니다.
    - 프로세스는 시스템 콜을 발생시켜 자신이 직접 인터럽트 작업을 수행하지 않고 운영체제가 이 작업을 수행하도록 합니다. 운영체제는 요청받은 시스템 콜을 `ISR(Interrupt Service Routine)` 에서 찾아 미리 정의된 시스템 콜 처리 작업을 수행합니다.
</details>

PCB에 대해 설명가능한지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - PCB는 프로세스에 대한 정보를 담고있는 자료구조 입니다. 운영체제는 이 PCB를 사용해서 프로세스를 관리합니다. PCB는 프로세스의 생성과 함께 만들어집니다.
    - PCB는 다음과 같은 정보를 담고 있습니다.
    1. 프로세스 ID
    2. 프로세스 상태
    3. 프로그램 카운터
    4. 계정정보
    5. 스케줄링 정보
    6. 부모 프로세스와 자녀 프로세스에 대한 포인터
    7. 레지스터 정보
    8. 입출력 상태
</details>

Context switch(문맥교환)에 대해 설명가능한지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 이때 레지스터 값을 저장하고 새로운 값으로 레지스터를 채우는 동안 CPU는 다른 작업을 할 수 없으므로 큰 `오버헤드` 가 발생합니다.
    - 컨텍스트 스위치는 CPU를 점유하던 프로세스가 인터럽트에 의해 다른 프로세스로 교체할 때, 기존 프로세스의 레지스터 값을 저장하고 새로운 프로세스의 레지스터값을 CPU 레지스터로 로드하는 것을 말합니다.
</details>

*Orphan 프로세스와 좀비 프로세스에대해 아는지?(희소)*
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - Orphan 프로세스는 자녀 프로세스가 종료되기 전에 부모 프로세스가 wait 시스템 콜로 기다리지 않고 종료된 상태를 말합니다. 리눅스 운영체제의 경우에는 Orphan 프로세스를 막기 위해 부모 프로세스가 먼저 종료된 자녀 프로세스를 루트 프로세스인 init 프로세스의 자식으로편입시키고 주기적으로 wiat 시스템콜을 호출하여 자녀 프로세스들의 상태를 회수합니다.
    - Zombie 프로세스는 자녀 프로세스가 종료되었지만 부모 프로세스가 해당 프로세스를 회수하지 못한 경우를 말합니다. 자녀 프로세스는 종료됐지만 부모 프로세스가 wait 을 하지 않는 상태에서 발생합니다.
</details>
좀비 프로세스는 무슨 문제가 되나요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 좀비 프로세스는 종료된 프로세스이기 때문에 CPU를 사용하지는 않지만 더 이상 사용하지 않는 리소스를 반환하지 않고 커널의 프로세스 테이블에 관리되어 불피요한 공간을 차지하게 됩니다
</details>
IPC와 프로세스 간 통신 방법을 알고있나요? 
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    
    프로세스는 독립적으로 실행된다. 즉, 독립 되어있다는 것은 다른 프로세스에게 영향을 받지 않는다고 말할 수 있다. (스레드는 프로세스 안에서 자원을 공유하므로 영향을 받는다)
    
    이런 독립적 구조를 가진 **프로세스 간의 통신**을 해야 하는 상황이 있을 것이다. 이를 가능하도록 해주는 것이 바로 IPC 통신이다.
    
    프로세스는 커널이 제공하는 IPC 설비를 이용해 프로세스간 통신을 할 수 있게 된다.
    
    ***커널이란?***
    
    > 운영체제의 핵심적인 부분으로, 다른 모든 부분에 여러 기본적인 서비스를 제공해줌
    > 
    
    IPC 설비 종류도 여러가지가 있다. 필요에 따라 IPC 설비를 선택해서 사용해야 한다.
    
    ### **IPC 종류**
    
    1. **익명 PIPE**
        
        > 파이프는 두 개의 프로세스를 연결하는데 하나의 프로세스는 데이터를 쓰기만 하고, 다른 하나는 데이터를 읽기만 할 수 있다.
        > 
        > 
        > **한쪽 방향으로만 통신이 가능한 반이중 통신**이라고도 부른다.
        > 
        > 따라서 양쪽으로 모두 송/수신을 하고 싶으면 2개의 파이프를 만들어야 한다.
        > 
        > 매우 간단하게 사용할 수 있는 장점이 있고, 단순한 데이터 흐름을 가질 땐 파이프를 사용하는 것이 효율적이다. 단점으로는 전이중 통신을 위해 2개를 만들어야 할 때는 구현이 복잡해지게 된다.
        > 
    2. **Named PIPE(FIFO)**
        
        > 익명 파이프는 통신할 프로세스를 명확히 알 수 있는 경우에 사용한다. (부모-자식 프로세스 간 통신처럼)
        > 
        > 
        > Named 파이프는 전혀 모르는 상태의 프로세스들 사이 통신에 사용한다.
        > 
        > 즉, 익명 파이프의 확장된 상태로 **부모 프로세스와 무관한 다른 프로세스도 통신이 가능한 것** (통신을 위해 이름있는 파일을 사용)
        > 
        > 하지만, Named 파이프 역시 읽기/쓰기 동시에 불가능함. 따라서 전이중 통신을 위해서는 익명 파이프처럼 2개를 만들어야 가능
        > 
    3. **Message Queue**
        
        > 입출력 방식은 Named 파이프와 동일함
        > 
        > 
        > 다른점은 메시지 큐는 파이프처럼 데이터의 흐름이 아니라 메모리 공간이다.
        > 
        > 사용할 데이터에 번호를 붙이면서 여러 프로세스가 동시에 데이터를 쉽게 다룰 수 있다.
        > 
    4. **공유 메모리**
        
        > 파이프, 메시지 큐가 통신을 이용한 설비라면, 공유 메모리는 데이터 자체를 공유하도록 지원하는 설비다.
        > 
        > 
        > 프로세스의 메모리 영역은 독립적으로 가지며 다른 프로세스가 접근하지 못하도록 반드시 보호되야한다. 하지만 다른 프로세스가 데이터를 사용하도록 해야하는 상황도 필요할 것이다. 파이프를 이용해 통신을 통해 데이터 전달도 가능하지만, 스레드처럼 메모리를 공유하도록 해준다면 더욱 편할 것이다.
        > 
        > 공유 메모리는 **프로세스간 메모리 영역을 공유해서 사용할 수 있도록 허용**해준다.
        > 
        > 프로세스가 공유 메모리 할당을 커널에 요청하면, 커널은 해당 프로세스에 메모리 공간을 할당해주고 이후 모든 프로세스는 해당 메모리 영역에 접근할 수 있게 된다.
        > 
        > - **중개자 없이 곧바로 메모리에 접근할 수 있어서 IPC 중에 가장 빠르게 작동함**
    5. **메모리 맵**
        
        > 공유 메모리처럼 메모리를 공유해준다. 메모리 맵은 열린 파일을 메모리에 맵핑시켜서 공유하는 방식이다. (즉 공유 매개체가 파일+메모리)
        > 
        > 
        > 주로 파일로 대용량 데이터를 공유해야 할 때 사용한다.
        > 
    6. **소켓**
        
        > 네트워크 소켓 통신을 통해 데이터를 공유한다.
        > 
        > 
        > 클라이언트와 서버가 소켓을 통해서 통신하는 구조로, 원격에서 프로세스 간 데이터를 공유할 때 사용한다.
        > 
        > 서버(bind, listen, accept), 클라이언트(connect)
        > 
    
    이러한 IPC 통신에서 프로세스 간 데이터를 동기화하고 보호하기 위해 세마포어와 뮤텍스를 사용한다. (공유된 자원에 한번에 하나의 프로세스만 접근시킬 때)
    
    - 여러 프로세스가 서로 통신하기 위해서는 공유메모리 방법이나 메세지 패싱 방법을 사용해야합니다.
    - 공유메모리는 한 프로세스에 공유 메모리를 설정하고 다른 프로세스들이 해당 메모리에 접근하여 버퍼를 통해 데이터를 공유하는 방식입니다.
    - 메세지 패싱 방법은 커널 영역에 통신을 위한 메일 박스를 만들어두고 이곳을 버퍼로 삼아 프로세스들이 데이터를 주고받습니다.
</details>
cpu 스케줄러가 뭐에요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - CPU 스케줄러는 Ready 큐에서 CPU에 로드되길 기다리는 프로세스들 중 어떤 프로세스를 로드할 것인지 결정합니다. `단기 스케줄러`라고도 합니다.
</details>
cpu에 장기 스케줄러가 있나요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 장기 스케줄러는 job 스케줄러라고 부릅니다. 메모리에 있는 프로세스 중 어떤 프로세스를 준비 큐에 넣을지 결정하는 스케줄러입니다. 또한 메모리에 올라와 있는 프로세스의 숫자를 제어해서 `degree of multiprogramming` 을 관리합니다.
</details>
다른 스케줄러도 있을까요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 중기스케줄러가 있습니다. 만약 메모리에 너무 많은 프로세스가 올라오게 되면 중기 스케줄러는 메모리에 있는 프로세스를 강제로 디스크의 스왑 영역에 저장합니다. 이를 swap-out 이라고 합니다.
</details>
멀티 태스킹과 멀티 프로세싱의 차이가 뭐에요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 멀티 태스킹은 하나의 프로세서(CPU 코어)가 운영체제의 스케줄링을 받아 여러 작업을 빠르게 번갈아가면서 실행하는 방식입니다.
    - 멀티 프로세싱은 여러 프로세서가 여러 작업을 병렬적으로 수행하는 것을 말합니다.
</details>
DMA의 존재 이유에 대해 아시는지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 모든 메모리 접근 연산이 CPU에 의해서만 이루어질 경우 주변 장치가 메모리 접근을 원할 때마다 인터럽트를 통해 CPU 업무가 방해를 받게 되어 CPU의 사용의 효율성이 떨어지는 문제가 발생한다.
    - DMA는 일종의 컨트롤러로서 CPU가 주변 장치들의 메모리 접근 요청에 의해 자주 인터럽트당하는 것을 막아주는 역할을 한다.
    - DMA를 사용하면 로컬 버퍼에서 메모리로 읽어오는 작업을 CPU가 담당하는 것이 아니라, DMA가 대행하므로서 CPU는 원래 하던 작업을 멈추고 인터럽트를 처리할 필요가 없어지는 것이다.
</details>
운영체제는 다중 유저가 하나의 컴퓨터의 자원을 사용할 때 자원의 '보호'를 합니다. 어떠한 보호를 하는지 설명하고 시나리오를 설명하세요
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    
    크게 세 부분으로 나눌 수 있습니다.
    
    **[1] 입출력장치 보호**
    
    - A가 프린터에 인쇄를 요청하여 프린터가 A의 작업을 수행 중일 때 B가 프린터 요청을 하면 A의 작업 이후에 B의 작업을 수행해야합니다.
    - 이와 관계된특권 명령(in, out) 명령은 에플리케이션에서 하는 것이 아닌 운영체제가 수행합니다.
    
    **[2] 메모리 보호**
    
    - A가 실행한 프로세스는 B가 실행한 프로세스의 메모리를 읽거나 쓰지 못하도록 막습니다.
    - CPU와 메모리 사이에 MMU(Memory Management Unit)두어서 base, limit 레지스터 값을 읽어서 해당 메모리 부분을 넘지 못하도록 합니다.
    
    **[3] CPU 보호**
    
    - while ( n = 1) 과 같이 실수 혹은 고의로 하나의 프로세스가 CPU시간을 독점하는 일을 방지해야합니다.
    - 일정 주기로 CPU에게 타이머가 인터럽트를 걸도록 회로를 설계합니다. 인터럽트를 걸면 CPU는 지금 하는 일을 멈추고 인터럽트 서비스 루틴으로 넘어갑니다. 이 코드에는 CPU 시간이 다른 모든 프로세스에게 골고루 가는지, 한 놈에게 집중되는지 체크합니다.
</details>
한컴오피스 '한글'을 클릭 후 빈 화면어 커서가 깜빡이고 있다. 이때 hello world를 작성하면 컴퓨터 내부에서 어떤일이 발생하는가?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 키보드에서 사용자 입력이 들어오면 키보드 컨트롤러가 인터럽트를 발생시켜 CPU에게 키가 입력되었다는 사실을 알려준다.
    - CPU는 현재 수행중이던 작업의 상태를 저장하고 인터럽트 요청을 처리하기 위해 OS내에 정의된 키보드 인터럽트 처리 루틴을 찾아간다.
    - 키보드 인터럽트 처리 루틴은 키보드로 부터 입력받은 내용을 메모리의 특정 부분에 저장해 해당 프로그램에게 키보드 입력이 들어왔음을 알리며 인터럽트 처리를 완료한다.
    - 인터럽트 처리가 끝나면 인터럽트가 발생하기 직전 상태를 복구시켜 중단되었던 작업을 재개한다.
</details>
---

선점형 스케줄링과 비선점형 스케줄링의 차이를 아나요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 선점형 스케줄링은 CPU에 할당된 프로세스가 작업이 완전히 끝나지 않아도 강제적으로 기존 프로세스를 새로운 프로세스로 교체해버리는 스케줄링 방법입니다.
    - 비선점형 스케줄링은 반대로 CPU에 할당된 프로세스의 작업이 완전히 끝난 이후에 CPU에 할당되는 프로세스를 교체하는 스케줄링 방법입니다.
</details>
FCFS 스케줄링 알고리즘 설명 가능한지
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - FCFS 알고리즘은 준비 큐에 먼저 도착한 프로세스를 CPU에 할당하는 프로세스로 선택하는 스케줄링 알고리즘입니다.
</details>
FCFS 많이 쓸까요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 아닙니다. FCFS 알고리즘은 항상 먼저 도착한 프로세스를 선택하기 때문에 만약 가장 늦게 도착한 프로세스가 얼마 걸리지 않는 작업을 수행하는 프로세스일 경우에 문제가 생깁니다. 해당 프로세스가 실제로 작업에 필요한 시간은 짧지만 실행을 위해 준비 큐에서 오랫동안 기다려야 하는 큰 취약점을 가지고 있습니다.
</details>
SJF 알고리즘은요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    
    JF 알고리즘은 수행시간이 가장 짧은 프로세스를 우선적으로 처리하여 준비 큐의 프로세스들의 대기시간을 최소화하는 알고리즘입니다. 따라서 운영체제의 스케줄링에 가장 적합한 알고리즘이라고 할 수 있습니다.
</details>

SJF 많이 쓸까요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 아닙니다. 실제로 어떤 프로세스의 수행시간을 실행하지 않고 알아낼 수 있는 방법이 없기 때문에 구현이 불가능한 알고리즘입니다. 하지만 다음 프로세스의 수행시간이 이전 수행시간과 비슷할 것이라는 가정을 하여 수학적 예측을 통해 구현할 수는 있습니다.
</details>
어떤 알고리즘이 주로 사용되는지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - Round Robin 이나 Priority 스케줄링을 함께 사용합니다. 라운드 로빈 스케줄링은 `time quantum` 이라고 부르는 타임스탬프를 사용해서 스케줄링을 합니다.
    - 운영체제는 준비 큐에 있는 프로세스들을 정확히 time quantum 시간만큼만 CPU에 할당시키고 시간이 끝나면 곧바로 다음 프로세스와 교체시킵니다. 이렇게 스케줄링을 하면 다수의 프로세스를 여러번에 나눠서 실행할 수 있기 때문에 반응속도가 중요한 시스템에서 유용하게 사용될 수 있습니다.
    - Priority 스케줄링은 준비 큐에 있는 프로세스들을 우선순위에 따라 CPU에 할당하는 알고리즘입니다. 새로운 프로세스가 준비 큐에 도착했을 때, 우선순위가 높다면 도착시간과는 관계없이 먼저 선택됩니다
</details>
라운드 로빈 알고리즘의 단점이 뭔가요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 라운드 로빈 알고리즘은 time quantum 에 의존적입니다. 만약 time quantum 이 준비 큐에 있는 각 프로세스의 수행시간보다 길다면, FCFS과 동일한 스케줄링이 됩니다. 반대로 time quantum 이 너무 짧다면, 프로세스의 컨텍스트 스위칭이 지나치게 많이 발생할 수 있습니다.
</details>
우선순위 스케줄리의 단점이 뭔가요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 우선순위에 따라 스케줄링을 하게되면 우선순위가 상대적으로 낮은 프로세스가 실행되지 못하고 장시간동안 준비 큐에 남아있을 수 있습니다. 이런 현상을 `Starvation` 혹은 `Indefinite Blocking` 이라고 합니다.
</details>
이 문제는 어떻게 해결할수있을지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 두 가지 방법이 있습니다. 첫번째는 `Aging` 기법을 사용하여 준비 큐에 오래 머무는 프로세스의 우선순위를 점진적으로 높여주는 방식입니다. 다른 방법은 라운드 로빈 알고리즘을 섞어서 사용하는 것입니다. 라운드 로빈은 공정하게 모든 프로세스에게 CPU를 점유하게 합니다. 우선순위에 따라 프로세스들이 정렬되고 이 프로세스들에 대해 라운드 로빈 알고리즘을 사용한다면 큐의 뒤에 위치한 프로세스도 CPU 점유의 기회를 받을 수 있게 됩니다.
</details>
멀티 레벨 큐에대해 들어봤어요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 멀티 레벨 큐는 준비 큐를 여러개의 큐로 구성하고 새로운 프로세스가 들어올 때마다 중요도에 따라 각각 다른 큐에 넣어 관리하는 알고리즘입니다. 각 큐는 서로 다른 스케줄링 알고리즘을 사용합니다. 하지만 각 프로세스가 한 큐에서 다른 큐로 이동할 수 없어 유연하지 못한 스케줄링 알고리즘입니다.
</details>
멀티 레벨 피드백 큐는 뭐에요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 멀티 레벨 피드백 큐는 여러 큐로 나눈 준비 큐의 상위 계층에 time quantum 을 두어 CPU 사용시간에 따라 프로세스의 위치를 바꿔주고 스케줄링 알고리즘을 단계별로 다르게 적용하는 알고리즘 입니다.
</details>
*실시간 스케줄링도 알아요?(희소)*
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 먼저, Rate Monotonic 스케줄링이 있습니다. 이 알고리즘은 프로세스의 우선순위에 따라 CPU에 프로세스를 스케줄링합니다. 이때, 우선순위는 각 프로세스가 가지는 CPU 요청 주기에 따라 주기가 짧은 프로세스가 높은 우선순위를 가지게됩니다.
    - 다음은 Earlist Deadline First 스케줄링입니다. 이 알고리즘은 각 프로세스의 마감시간을 기준으로 우선순위를 부여합니다. 마감시간이 빠르면 높은 우선순이가 부여됩니다. 각 프로세스들은 자신이 실행가능한 상태가 되면 시스템의 자신의 마감시간을 알리고 이 마감시간으로 우선순위를 부여하게됩니다. 이 알고리즘은 이론적으로는 완벽하지만 프로세스가 교체되면서 발생하는 컨텍스트 스위칭의 오버헤드 때문에 딜레이가 생겨 실제로는 구현이 불가능합니다.
</details>

---

스레드가 무엇인지 설명해주세요
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 스레드는 프로세스가 수행하는 작업의 실행단위입니다. 스레드는 프로세스 내부에 존재하며 프로세스가 할당받은 자원을 사용하여 작업을 수행합니다.
</details>
스레드랑 프로세스는 무슨 차이에요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 스레드는 프로세스 내에서 실행됩니다. 한 프로세스는 여러 스레드를 가질 수 있고, 이 스레드들은 해당 프로세스의 자원을 일부 서로 공유합니다. 프로세스는 생성시 코드영역, 데이터영역, 힙영역, 스택영역을 할당받습니다. 스레드는 여기서 스택 영역만 각 스레드가 독립적으로 유지하고 나머지 영역들을 프로세스 내의 스레드들이 함께 공유합니다.
</details>
각 스레드는 왜 독립적인 스택 영역을 가질까요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 스택영역을 따로 관리하기에 각 스레드는 서로 다른 작업을 수행할 수 있습니다. 함수 호출 시 한 스레드가 호출한 함수에 대한 스택정보가 다른 스레드에 영향을 준다면 스레드를 사용하는 의미가 없을 것입니다. 각 스레드는 프로세스의 작업 흐름을 담당해야하므로 서로 다른 작업흐름을 가지는 것이 허용되어야 합니다. 이를 위해 스택영역을 독립적으로 가지고 이와 더불어서 프로그램 카운터 레지스터역시도 독립적으로 가지게 됩니다.
</details>
각 스레드는 스택만 독립으로 가지나요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    
    PC 레지스터도 독립할당이다. PC 값은 스레드가 명령어의 어디까지 수행하였는지를 나타나게 된다. 스레드는 CPU 를 할당받았다가 스케줄러에 의해 다시 선점당한다. 그렇기 때문에 명령어가 연속적으로 수행되지 못하고 어느 부분까지 수행했는지 기억할 필요가 있다. 따라서 PC 레지스터를 독립적으로 할당한다.
</details>

멀티 스레딩이 뭐고 왜 사용하나요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 멀티 스레딩은 하나의 프로그램을 여러개의 스레드로 구성해서 여러 스레드가 한 프로세스를 병렬적으로 실행할 수 있도록 하는 방법입니다. 한 프로세스 내부의 스레드는 서로 힙 영역을 공유하기 때문에 별다른 통신 기술을 사용하지 않고도 스레드끼리 데이터를 공유할 수 있습니다. 또한 스레드는 서로 공유하는 영역이 많기 때문에 CPU 캐시의 hit 율이 더 높고 이 때문에 멀티프로세싱보다 컨텍스트 스위칭의 비용이 더 저렴합니다.
</details>
멀티 프로세스 대신 멀티 스레드를 사용하는 이유는?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 쉽게 설명하면, 프로그램을 여러 개 키는 것보다 하나의 프로그램 안에서 여러 작업을 해결하는 것이다.
        - 
            
            ![https://github.com/WeareSoft/tech-interview/raw/master/contents/images/multi-thread.png](https://github.com/WeareSoft/tech-interview/raw/master/contents/images/multi-thread.png)
            
    1. 자원의 효율성 증대
        - 멀티 프로세스로 실행되는 작업을 멀티 스레드로 실행할 경우, **프로세스를 생성하여 자원을 할당하는 시스템 콜이 줄어들어** 자원을 효율적으로 관리할 수 있다.
            - 프로세스 간의 Context Switching시 단순히 CPU 레지스터 교체 뿐만 아니라 RAM과 CPU 사이의 캐시 메모리에 대한 데이터까지 초기화되므로 오버헤드가 크기 때문
        - 스레드는 프로세스 내의 메모리를 공유하기 때문에 독립적인 프로세스와 달리 스레드 간 데이터를 주고 받는 것이 간단해지고 시스템 자원 소모가 줄어들게 된다.
    2. 처리 비용 감소 및 응답 시간 단축
        - 또한 프로세스 간의 통신(IPC)보다 스레드 간의 통신의 비용이 적으므로 작업들 간의 통신의 부담이 줄어든다.
            - 스레드는 Stack 영역을 제외한 모든 메모리를 공유하기 때문
        - 프로세스 간의 전환 속도보다 스레드 간의 전환 속도가 빠르다.
            - Context Switching시 스레드는 Stack 영역만 처리하기 때문
    - ***주의할 점!***
        - **동기화 문제**
        - 스레드 간의 자원 공유는 전역 변수(데이터 세그먼트)를 이용하므로 함께 상용할 때 충돌이 발생할 수 있다.
</details>
*사용자 수준 스레드와 커널 수준 스레드의 차이가 있을까요?(회소)*
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 사용자 수준 스레드는 사용자 영역의 스레드 라이브러리를 통해 구현됩니다. 모든 구현과 동작은 사용자 영역에서 진행되기 때문에 커널은 사용자 수준 스레드의 존재유무를 알지 못합니다.
    - 커널 수준 스레드는 운영체제가 생성하고 관리하는 스레드입니다.
</>
멀티 스레딩에는 문제가 없나요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 멀티 스레딩을 사용하게 되면 모든 스레드들이 데이터 영역, 힙 영역, 코드 영역을 공유하게 됩니다. 이 때 다수의 스레드가 데이터 영역과 힙 영역의 데이터에 동시에 접근하게 되면 동기화 문제가 발생합니다. 이를 해결하기 위해 Mutex 나 Semaphore 같은 방법을 사용합니다.
</details>
---

MMU에 대해 알고 있을까요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - MMU 는 CPU 와 메모리 사이에서 주소의 영역을 설정하는 역할을 합니다. MMU 내부에는 재배치 레지스터가 존재해서 CPU가 요구하는 메모리의 위치와 실제 메모리상의 프로그램의 위치를 맞춰줍니다. 즉, 논리적 주소를 물리적 주소로 변환하는 작업을 합니다.
</details>
가상 메모리는 뭐고 무슨 일을 하나요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    
    가상메모리는 **프로세스 전체가 메모리 내에 올라오지 않더라도 실행이 가능하도록 하는 기법** 이며, 프로그램이 물리 메모리보다 커도 된다는 주요 장점이 있다.
    
    가상 메모리 이전에는 실행되는 **코드의 전부를 물리 메모리에 존재시켜야** 했고, **메모리 용량보다 큰 프로그램은 실행시킬 수 없었다.** 또한, 여러 프로그램을 동시에 메모리에 올리기에는 용량의 한계와, 페이지 교체등의 성능 이슈가 발생하게 된다. 또한, 가끔만 사용되는 코드가 차지하는 메모리들을 확인할 수 있다는 점에서, 불필요하게 전체의 프로그램이 메모리에 올라와 있어야 하는게 아니라는 것을 알 수 있다
    
    ### **프로그램의 일부분만 메모리에 올릴 수 있다면...**
    
    - 물리 메모리 크기에 제약받지 않게 된다.
    - 더 많은 프로그램을 동시에 실행할 수 있게 된다. 이에 따라 `응답시간`은 유지되고, `CPU 이용률`과 `처리율`은 높아진다.
    - [swap](https://github.com/JaeYeopHan/Interview_Question_for_Beginner/tree/master/OS#%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B4%80%EB%A6%AC-%EB%B0%B0%EA%B2%BD)에 필요한 입출력이 줄어들기 때문에 프로그램들이 빠르게 실행된다.
    
    ### **가상 메모리가 하는 일**
    
    가상 메모리는 실제의 물리 메모리 개념과 사용자의 논리 메모리 개념을 분리한 것으로 정리할 수 있다. 이로써 작은 메모리를 가지고도 얼마든지 큰 `가상 주소 공간`을 프로그래머에게 제공할 수 있다.
    
    가상 메모리는...
    
    - `시스템 라이브러리`가 여러 프로세스들 사이에 공유될 수 있도록 한다. 각 프로세스들은 `공유 라이브러리`를 자신의 가상 주소 공간에 두고 사용하는 것처럼 인식하지만, 라이브러리가 올라가있는 `물리 메모리 페이지`들은 모든 프로세스에 공유되고 있다.
    - 프로세스들이 메모리를 공유하는 것을 가능하게 하고, 프로세스들은 공유 메모리를 통해 통신할 수 있다. 이 또한, 각 프로세스들은 각자 자신의 주소 공간처럼 인식하지만, 실제 물리 메모리는 공유되고 있다.
    - `fork()`를 통한 프로세스 생성 과정에서 페이지들이 공유되는 것을 가능하게 한다.
</details>

Swapping은 무엇인가요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 스와핑은 현재 실행 중이지 않은 메모리에 올라온 프로세스를 메모리 공간을 효율적으로 사용하기 위해 디스크에 임시로 저장하고 필요할 때 다시 불러오는 방법입니다.
</details>
Swapping의 문제점은요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 임시 저장을 디스크에 하기 때문에 스왑의 속도가 오래걸립니다. 따라서 실제로 사용할 때는 전체 프로세스를 스와핑하는 것이 아니라 프로세스의 일부인 페이지만 스왑하거나 디스크내의 파일시스템과 분리된 스왑공간을 만드는 방식으로 구현합니다.
</details>
연속 메모리 할당법에 대해 알고있나요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 최초 적합, 최적 적합, 최악 적합으로 세 가지 방법이 있습니다.
    - 최초 적합은 메모리의 시작이나 끝지점부터 탐색하면서 요청받은 프로세스를 할당할 수 있는 공간이 발견되면 곧바로 프로세스를 할당하는 방법입니다.
    - 최적 적합은 전체 메모리에 남은 공간을 다 확인해보고 요청받은 프로세스를 할당할 수 있는 가장 작은 공간에 프로세스를 할당하는 방법입니다.
    - 최악 적합은 전체 메모리에 남은 공간을 다 확인해보고 요청받은 프로세스를 할당할 수 있는 가장 큰 공간에 프로세스를 할당하는 방법입니다.
</details>
세그멘테이션과 페이징에 대해 설명가능한지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    
    ### **메모리 관리 배경**
    
    각각의 **프로세스** 는 독립된 메모리 공간을 갖고, 운영체제 혹은 다른 프로세스의 메모리 공간에 접근할 수 없는 제한이 걸려있다. 단지, **운영체제** 만이 운영체제 메모리 영역과 사용자 메모리 영역의 접근에 제약을 받지 않는다.
    
    **Swapping** : 메모리의 관리를 위해 사용되는 기법. 표준 Swapping 방식으로는 round-robin 과 같은 스케줄링의 다중 프로그래밍 환경에서 CPU 할당 시간이 끝난 프로세스의 메모리를 보조 기억장치(e.g. 하드디스크)로 내보내고 다른 프로세스의 메모리를 불러 들일 수 있다.
    
    > 이 과정을 swap (스왑시킨다) 이라 한다. 주 기억장치(RAM)으로 불러오는 과정을 swap-in, 보조 기억장치로 내보내는 과정을 swap-out 이라 한다. swap 에는 큰 디스크 전송시간이 필요하기 때문에 현재에는 메모리 공간이 부족할때 Swapping 이 시작된다.
    > 
    
    **단편화** (**Fragmentation**) : 프로세스들이 메모리에 적재되고 제거되는 일이 반복되다보면, 프로세스들이 차지하는 메모리 틈 사이에 사용 하지 못할 만큼의 작은 자유공간들이 늘어나게 되는데, 이것이 **단편화** 이다. 단편화는 2 가지 종류로 나뉜다.
    
    ```
    Process A
    ```
    
    ```
    Process B
    ```
    
    ```
    Process C
    ```
    
    ```
    Process D
    ```
    
    - 외부 단편화: 메모리 공간 중 사용하지 못하게 되는 일부분. 물리 메모리(RAM)에서 사이사이 남는 공간들을 모두 합치면 충분한 공간이 되는 부분들이 **분산되어 있을때 발생한다고 볼 수 있다.**
    - 내부 단편화: 프로세스가 사용하는 메모리 공간 에 포함된 남는 부분. 예를들어 **메모리 분할 자유 공간이 10,000B 있고 Process A 가 9,998B 사용하게되면 2B 라는 차이** 가 존재하고, 이 현상을 내부 단편화라 칭한다.
    
    압축 : 외부 단편화를 해소하기 위해 프로세스가 사용하는 공간들을 한쪽으로 몰아, 자유공간을 확보하는 방법론 이지만, 작업효율이 좋지 않다. (위의 메모리 현황이 압축을 통해 아래의 그림 처럼 바뀌는 효과를 가질 수 있다)
    
    ```
    Process A
    ```
    
    ```
    Process B
    ```
    
    ```
    Process C
    ```
    
    ```
    Process D
    ```
    
    다중 프로그래밍 시스템에 여러 프로세스를 수용하기 위해 주기억장치를 동적 분할하는 메모리 관리 작업이 필요하게됨.
    
    ### **Paging(페이징)**
    
    하나의 프로세스가 사용하는 메모리 공간이 연속적이어야 한다는 제약을 없애는 메모리 관리 방법이다. 외부 단편화와 압축 작업을 해소 하기 위해 생긴 방법론으로, 물리 메모리는 Frame 이라는 고정 크기로 분리되어 있고, 논리 메모리(프로세스가 점유하는)는 페이지라 불리는 고정 크기의 블록으로 분리된다.(페이지 교체 알고리즘에 들어가는 페이지)
    
    페이징 기법을 사용함으로써 논리 메모리는 물리 메모리에 저장될 때, 연속되어 저장될 필요가 없고 물리 메모리의 남는 프레임에 적절히 배치됨으로 외부 단편화를 해결할 수 있는 큰 장점이 있다.
    
    하나의 프로세스가 사용하는 공간은 여러개의 페이지로 나뉘어서 관리되고(논리 메모리에서), 개별 페이지는 **순서에 상관없이** 물리 메모리에 있는 프레임에 mapping 되어 저장된다고 볼 수 있다.
    
    - 단점 : 내부 단편화 문제의 비중이 늘어나게 된다. 예를들어 페이지 크기가 1,024B 이고 **프로세스 A** 가 3,172B 의 메모리를 요구한다면 3 개의 페이지 프레임(1,024 * 3 = 3,072) 하고도 100B 가 남기때문에 총 4 개의 페이지 프레임이 필요한 것이다. 결론적으로 4 번째 페이지 프레임에는 924B(1,024 - 100)의 여유 공간이 남게 되는 내부 단편화 문제가 발생하는 것이다.
    
    ### **Segmentation(세그멘테이션)**
    
    페이징에서처럼 논리 메모리와 물리 메모리를 같은 크기의 블록이 아닌, 서로 다른 크기의 논리적 단위인 세그먼트(Segment)로 분할 사용자가 두 개의 주소로 지정(세그먼트 번호 + 변위) 세그먼트 테이블에는 각 세그먼트의 기준(세그먼트의 시작 물리 주소)과 한계(세그먼트의 길이)를 저장
    
    - 단점 : 서로 다른 크기의 세그먼트들이 메모리에 적재되고 제거되는 일이 반복되다 보면, 자유 공간들이 많은 수의 작은 조각들로 나누어져 못 쓰게 될 수도 있다.(외부 단편화)
</details>

메모리 단편화에 대해 설명해주시겠어요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 메모리 단편화는 내부 단편화와 외부 단편화 두 종류가 있습니다.
    - 외부 단편화는 프로세스를 연속 메모리 할당할 때, 흩어진 빈 공간을 합치면 프로세스를 로드할 수 있는 만큼의 여유공간이 있지만 각각의 빈 공간들이 요청받은 프로세스를 할당할 수 있는 크기를 가지지 못해 프로세스를 로드할 수 없는 상태를 의미합니다.
    - 내부 단편화는 프로세스를 동일한 크기의 페이지로 분할 할 때, 페이지를 모두 다 사용하지 않고 마지막 페이지의 일부만을 사용하여 페이지 내부에 빈 공간이 남게되는 상태를 의미합니다.
</details>

페이지 테이블에 대해 설명해주세요
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 페이지 테이블은 한 프로세스에 대한 논리 주소와 물리 주소를 맵핑한 정보를 가진 테이블입니다. 페이지 테이블은 메인 메모리에 저장되고 각 프로세스들은 `PTBR (Page Table Base Register)` 를 사용해서 페이지 테이블의 포인터를 저장합니다. 따라서 프로세스가 변경될 때는 페이지 테이블을 교체하는 것이 아니라 포인터의 주소만 바꿔서 오버헤드를 최소화합니다.
</details>

TLB를 왜 쓰고 어떻게 쓰는지 아시나요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    이지 테이블의 관점에서 CPU가 요청하는 주소가 실제 페이지까지 도달하기 위해서는 메모리를 두 번 읽어야 합니다. 먼저 레지스터에 저장된 페이지 테이블 포인터를 읽어 메인 메모리에 있는 페이지 테이블을 읽고, 페이지 테이블에 맵핑된 물리주소에 접근해 값을 가져올 때 한 번 더 메모리를 읽게 됩니다. 이때 발생하는 지연시간을 최소화하기 위해서 TLB를 사용합니다.
    
    - CPU 요청하는 주소를 찾기위해 바로 페이지 테이블에 접근하지 않고 TLB에 해당 논리주소에 대한 맵핑 주소가 있는지 확인합니다. 만약 맵핑정보가 존재한다면 페이지 테이블을 거치지 않고 곧바로 맵핑된 물리 주소에 접근합니다. 만약 정보가 없다면 페이지 테이블을 읽어 물리주소를 확인한 뒤 작업을 수행합니다.
</details>

공유 페이지에 대해 아시나요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 공유페이지는 똑같은 코드영역을 사용하는 다수의 프로세스가 있을 때 하나의 페이지만 유지하고 각 프로세스의 페이지 테이블이 해당 페이지를 가르키게 하는 기법입니다. 이를 통해 여러 프로세스가 중복으로 사용하는 페이지를 하나만 유지하여 메모리 공간을 절약할 수 있습니다.
</details>

요구 페이징에 대해 말해주세요
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 요구 페이징은 프로세스의 페이지를 모두 메모리에 로드해서 사용하는 것이 아니라 당장 사용할 페이지만 로드하여 사용하는 방법입니다. 요구페이징은 페이지 테이블의 엔트리에 `valid bit` 를 추가하는 것으로 구현합니다. 만약 CPU가 접근하려는 페이지가 메모리에 없다면 `invalid bit` 를 표시하고 메모리에 존재한다면 `valid bit` 로 표시합니다.
</details>

페이지 폴트가 일어났을때 어떤일이 일어나죠?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    
    PU가 요청한 논리주소가 페이지 테이블 상에 invalid 비트로 설정되어 있다면 페이지 테이블이 CPU로 인터럽트를 보냅니다. 인터럽트를 받은 CPU는 운영체제에게 Page Fault Routine 을 요구합니다. 운영체제는 backing store 혹은 스왑 공간에서 CPU 가 원하는 페이지를 메모리로 가져옵니다. 그리고 페이지 테이블의 valid bit 를 `valid` 로 바꿔줍니다.
</details>

지역성의 원리에 대해 아시나요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 지역성의 원리는 시간적 지역성과 공간적 지역성이 있습니다. 시간적 지역성은 최근에 참조한 메모리주소를 다시 참조할 가능성이 높다는 것이고, 공간적 지역성은 어떤 메모리 주소를 참조했을 때 다음에 참조할 메모리는 해당 주소 근처에 있을 확률이 높다는 것을 의미합니다.
    
    부연 설명하자면 
    
    캐시 메모리는 속도가 빠른 장치와 느린 장치간의 속도차에 따른 병목 현상을 줄이기 위한 범용 메모리이다. 이러한 역할을 수행하기 위해서는 CPU 가 어떤 데이터를 원할 것인가를 어느 정도 예측할 수 있어야 한다. 캐시의 성능은 작은 용량의 캐시 메모리에 CPU 가 이후에 참조할, 쓸모 있는 정보가 어느 정도 들어있느냐에 따라 좌우되기 때문이다.
    
    이 때 `적중율(Hit rate)`을 극대화 시키기 위해 데이터 `지역성(Locality)의 원리`를 사용한다. 지역성의 전제조건으로 프로그램은 모든 코드나 데이터를 균등하게 Access 하지 않는다는 특성을 기본으로 한다. 즉, `Locality`란 기억 장치 내의 정보를 균일하게 Access 하는 것이 아닌 어느 한 순간에 특정 부분을 집중적으로 참조하는 특성인 것이다.
    
    이 데이터 지역성은 대표적으로 시간 지역성(Temporal Locality)과 공간 지역성(Spatial Locality)으로 나뉜다.
    
    - 시간 지역성 : 최근에 참조된 주소의 내용은 곧 다음에 다시 참조되는 특성.
    - 공간 지역성 : 대부분의 실제 프로그램이 참조된 주소와 인접한 주소의 내용이 다시 참조되는 특성
    
    ### **Caching line**
    
    언급했듯이 캐시(cache)는 프로세서 가까이에 위치하면서 빈번하게 사용되는 데이터를 놔두는 장소이다. 하지만 캐시가 아무리 가까이 있더라도 찾고자 하는 데이터가 어느 곳에 저장되어 있는지 몰라 모든 데이터를 순회해야 한다면 시간이 오래 걸리게 된다. 즉, 캐시에 목적 데이터가 저장되어 있다면 바로 접근하여 출력할 수 있어야 캐시가 의미 있어진다는 것이다.
    
    그렇기 때문에 캐시에 데이터를 저장할 때 특정 자료구조를 사용하여 `묶음`으로 저장하게 되는데 이를 **캐싱 라인** 이라고 한다. 프로세스는 다양한 주소에 있는 데이터를 사용하므로 빈번하게 사용하는 데이터의 주소 또한 흩어져 있다. 따라서 캐시에 저장하는 데이터에는 데이터의 메모리 주소 등을 기록해 둔 태그를 달아놓을 필요가 있다. 이러한 태그들의 묶음을 캐싱 라인이라고 하고 메모리로부터 가져올 때도 캐싱 라인을 기준으로 가져온다. 종류로는 대표적으로 세 가지 방식이 존재한다.
    
    1. Full Associative
    2. Set Associative
    3. Direct Map
</details>

페이지 교체 알고리즘에 대해 아시는지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 가장 먼저 메모리에 올라왔던 페이지를 디스크로 내려보내는 FIFO 알고리즘이 있습니다.
    - 현재 메모리에 있는 프로세스 중 가장 오랫동안 사용되지 않을 페이지를 선택하는 OPT 알고리즘이 있지만 SJF 스케줄링 알고리즘과 마찬가지로 구현이 불가능합니다.
    - 일정 주기동안 참조횟수가 가장 적은 페이지를 선택하는 LFU 알고리즘이 있습니다.
    - 마지막으로 가장 오랫동안 참조되지 않은 페이지를 선택하는 LRU 알고리즘이 있습니다.
</details>
---

Race condition(경쟁상태)에 대해 아시는거 설명해주세요
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - Race Condition은 스레드가 데이터에 어떤 순서로 접근하는지에 따라 실행결과가 달라지는 상황을 말합니다.
</details>

임계구역이 무엇인지 또 해결을 위한 필수 조건을 아시는지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 임계구역은 어떤 코드의 영역에서 다수의 프로세스가 서로 데이터를 공유하는 부분을 말합니다. 한번에 여러 스레드가 임계구역에 접근하여 데이터를 변경하면 동기화 문제가 발생합니다.
    - 임계구역 문제를 해결하기 위해서는 세 가지 요구조건을 만족시켜야 합니다.
        1. `상호배제` : 어떤 프로세스가 임계 구역에 있다면 다른 스레드는 임계구역으로 진입할 수 없습니다.
        2. `진행` : 임계구역에 스레드가 없고 다수의 스레드가 임계구역 진입을 위해 대기 중이라면 하나의 스레드를 적절하게 골라 임계구역에 진입시켜야합니다.
        3. `한정 대기` : 한번 임계구역에 진입했던 스레드는 다음 실행에 대해 제한을 받아야합니다.
</details>

임계구역을 해결하기위한 하드웨어적 방법은 뭐가 있나요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - `Test & Set` 과 `Compare-and-Swap` 이 있습니다. 두 방법 모두 임계구역 진입을 위한 원자적인 연산을 보장합니다. 이를 통해 상호배제와 진행 조건을 만족시킵니다. 하지만 한번 임계구역에 진입했던 스레드가 다시 임계구역으로 재진입하는 것을 제한하지 않기 때문에 한정 대기 요구조건을 만족시키지는 못합니다.
</details>

세마포어는 무엇인가요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 세마포어는 wait, signal 의 원자적 명령을 사용해서 공유자원 문제를 해결한다. 세마포어는 정수값을 가지고 이 값이 임계구역 내에 접근이 가능한 자원의 개수를 의미합니다. 어떤 스레드가 자원을 획득하면 세마포어의 값이 감소하고, 방출하면 세마포어의 값이 증가합니다.
</details>

세마포어의 단점은?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 세마포어는 스레드가 임계구역에 자리가 생길 때까지 무한루프를 돌며 기다리게 하는 스핀락으로 구현되어 있습니다. 이 때문에 불필요한 CPU Time 을 사용한다는 단점이 있습니다.
</details>

뮤택스는 무엇인가요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 뮤텍스는 lock 을 사용하여 한번에 하나의 스레드만 뮤텍스 락을 획득하고 임계구역에 진입하게 하는 기술입니다.
</details>

뮤택스랑 세마포어는 무슨 차이가 있죠?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 뮤텍스는 락을 획득한 스레드만이 락을 방출할 수 있는 반면에 세마포어는 signal 함수에 의해 락을 획득하고 있지 않은 스레드도 세마포어의 값을 증가시켜 다른 스레드를 임계구역으로 진입시킬 수 있습니다.
</details>

데드락이 뭔가요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 데드락은 프로세스나 스레드가 서로의 자원을 얻기 위해 무한정하게 대기하고 있는 상태를 말합니다.
</details>

데드락의 발생 조건은 뭔가요?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 네 가지 조건이 있습니다.
        1. `상호 배제` : 한 스레드만 자원에 접근할 수 있습니다.
        2. `점유 대기` : 이미 하나의 자원을 점유하고 있는 상태에서 다른 스레드에 할당된 자원을 점유하려는 스레드가 존재해야합니다.
        3. `비선점` : 한 스레드에 할당된 자원은 스스로 방출할 때까지 선점될 수 없습니다.
        4. `순환대기` : 자원을 점유하려는 스레드들이 순환형태로 자원 점유를 위헤 대기해야합니다.
</details>

데드락을 해결할 수 있는 방법을 말해주세요.
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 네 가지 방법이 있습니다.
        1. `예방` : 교착 상태가 발생하는 네 조건 중 하나만 해결합니다.
        2. `회피` : 자원 할당 그래프나 은행원 알고리즘을 통해 교착상태가 발생하지 않도록 합니다.
        3. `회복` : 발생한 교착상태를 해결합니다.
        4. `무시` : 교착상태를 무시합니다.
</details>

자원 할당 그래프 알고리즘에대해 간략하게 설명가능한지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 자원 할당 그래프 알고리즘은 각 스레드와 공유자원을 그래프의 정점으로 만듭니다. 한 스레드가 다른 스레드에게 요청을 할 때는 요청간선을 두 스레드 사이에 만듭니다. 그리고 한 스레드가 자원을 획득하면 할당선을 만들게 됩니다. 이때 각 정점들 사이에 사이클이 발생하면 교착상태가 발생했다고 판단합니다.
</details>

은행원 알고리즘에 대해 설명가능한지?
<details>
   <summary> 답안 (👈 Click)</summary>
<br/>
- 답
    - 은행원 알고리즘은 안전상태와 불안전상태 개념을 사용합니다. 안전상태는 스레드가 요구한만큼 자원할당이 가능한 상태를 말하고 이때는 교착상태가 발생하지 않습니다. 불안전상태는 요구받은 만큼의 자원을 가지고 있지 않은 상태를 말합니다. 운영체제는 자원 할당 요청을 받았을 때 해당 자원을 할당한 이후에 안전상태가 유지되는지를 미리 검사합니다. 그리고 안전상태가 유지되는 자원 요구만을 수락하고 불안정 상태를 만드는 요청은 모두 거절합니다.
</details>
